#!/usr/bin/env python3
"""
æ¼”ç¤ºé‡è¯•åŠŸèƒ½çš„è„šæœ¬
"""

from scraper import XinfadiScraper
from config import Config
from loguru import logger

def main():
    """æ¼”ç¤ºé‡è¯•åŠŸèƒ½"""
    
    logger.info("=== é‡è¯•åŠŸèƒ½æ¼”ç¤º ===")
    logger.info(f"é…ç½®ä¿¡æ¯:")
    logger.info(f"  - æœ€å¤§é‡è¯•æ¬¡æ•°: {Config.RETRY_MAX_ATTEMPTS}")
    logger.info(f"  - é‡è¯•é—´éš”: {Config.RETRY_INTERVAL} ç§’")
    logger.info(f"  - è¯·æ±‚è¶…æ—¶: {Config.REQUEST_TIMEOUT} ç§’")
    logger.info("")
    
    # åˆ›å»ºæŠ“å–å™¨å®ä¾‹
    with XinfadiScraper() as scraper:
        logger.info("ğŸš€ å¼€å§‹æµ‹è¯•æ­£å¸¸è¯·æ±‚...")
        
        # æµ‹è¯•æ­£å¸¸è¯·æ±‚ï¼ˆè·å–æœ€æ–°1æ¡æ•°æ®ï¼‰
        result = scraper.scrape_page(limit=1, current=1)
        
        if result:
            records = result.get('list', [])
            total_count = result.get('count', 0)
            
            logger.info(f"âœ… è¯·æ±‚æˆåŠŸ!")
            logger.info(f"  - æ€»è®°å½•æ•°: {total_count}")
            logger.info(f"  - æœ¬é¡µè®°å½•æ•°: {len(records)}")
            
            if records:
                first_record = records[0]
                logger.info(f"  - ç¤ºä¾‹æ•°æ®: {first_record.get('prod_name', 'N/A')} - {first_record.get('avg_price', 'N/A')}å…ƒ")
        else:
            logger.error("âŒ è¯·æ±‚å¤±è´¥")
    
    logger.info("")
    logger.info("=== æ¼”ç¤ºå®Œæˆ ===")
    logger.info("")
    logger.info("é‡è¯•æœºåˆ¶è¯´æ˜:")
    logger.info("1. å½“é‡åˆ°ç½‘ç»œè¿æ¥é”™è¯¯ã€è¶…æ—¶é”™è¯¯æˆ–HTTPé”™è¯¯æ—¶ï¼Œä¼šè‡ªåŠ¨é‡è¯•")
    logger.info("2. æ¯æ¬¡é‡è¯•å‰ä¼šç­‰å¾…10ç§’")
    logger.info("3. æœ€å¤šé‡è¯•5æ¬¡ï¼Œå¦‚æœéƒ½å¤±è´¥åˆ™è¿”å›None")
    logger.info("4. JSONè§£æé”™è¯¯ç­‰éç½‘ç»œé”™è¯¯ä¸ä¼šé‡è¯•")
    logger.info("5. é‡è¯•è¿‡ç¨‹ä¸­ä¼šæœ‰è¯¦ç»†çš„æ—¥å¿—è¾“å‡º")

if __name__ == '__main__':
    main()