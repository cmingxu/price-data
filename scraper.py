import requests
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from ratelimit import limits, sleep_and_retry
from loguru import logger
from config import Config

class XinfadiScraper:
    """æ–°å‘åœ°ä»·æ ¼æ•°æ®æŠ“å–å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(Config.REQUEST_HEADERS)
        self.session.headers['Referer'] = Config.XINFADI_REFERER
        self.api_url = Config.XINFADI_API_URL
        
        # è®¾ç½®è¯·æ±‚è¶…æ—¶
        self.timeout = Config.REQUEST_TIMEOUT
        
        logger.info("æ–°å‘åœ°æ•°æ®æŠ“å–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    @sleep_and_retry
    @limits(calls=Config.RATE_LIMIT_CALLS, period=Config.RATE_LIMIT_PERIOD)
    def _make_request(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å‘é€POSTè¯·æ±‚ï¼Œå¸¦æœ‰é€Ÿç‡é™åˆ¶å’Œé‡è¯•æœºåˆ¶"""
        
        # Debug: æ˜¾ç¤ºçœŸå®è¯·æ±‚è·¯å¾„å’Œå‚æ•°
        logger.info(f"ğŸŒ Real request path: {self.api_url}")
        logger.info(f"ğŸ“‹ Request parameters: {data}")
        
        for attempt in range(Config.RETRY_MAX_ATTEMPTS):
            try:
                logger.info(f"ğŸ”„ Attempt {attempt + 1}/{Config.RETRY_MAX_ATTEMPTS}")
                
                response = self.session.post(
                    self.api_url,
                    data=data,
                    timeout=self.timeout
                )
                
                response.raise_for_status()
                
                # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '')
                if 'application/json' not in content_type:
                    logger.warning(f"å“åº”å†…å®¹ç±»å‹ä¸æ˜¯JSON: {content_type}")
                
                result = response.json()
                logger.info(f"âœ… Request successful, returned {len(result.get('list', []))} records")
                
                return result
                
            except (requests.exceptions.ConnectionError, 
                    requests.exceptions.Timeout, 
                    requests.exceptions.HTTPError) as e:
                # ç½‘ç»œç›¸å…³é”™è¯¯ï¼Œéœ€è¦é‡è¯•
                is_last_attempt = attempt == Config.RETRY_MAX_ATTEMPTS - 1
                
                if is_last_attempt:
                    logger.error(f"âŒ Network error after {Config.RETRY_MAX_ATTEMPTS} attempts: {e}")
                    return None
                else:
                    logger.warning(f"âš ï¸  Network error on attempt {attempt + 1}: {e}")
                    logger.info(f"â³ Retrying in {Config.RETRY_INTERVAL} seconds...")
                    time.sleep(Config.RETRY_INTERVAL)
                    
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")
                return None
            except Exception as e:
                logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
                return None
        
        return None
    
    def scrape_page(self, 
                   limit: int = 20,
                   current: int = 1,
                   pub_date_start_time: str = "",
                   pub_date_end_time: str = "",
                   prod_pcatid: str = "",
                   prod_catid: str = "",
                   prod_name: str = "") -> Optional[Dict[str, Any]]:
        """æŠ“å–å•é¡µæ•°æ®
        
        Args:
            limit: æ¯é¡µè®°å½•æ•°
            current: é¡µç 
            pub_date_start_time: å¼€å§‹æ—¥æœŸ
            pub_date_end_time: ç»“æŸæ—¥æœŸ
            prod_pcatid: äº§å“çˆ¶åˆ†ç±»ID
            prod_catid: äº§å“åˆ†ç±»ID
            prod_name: äº§å“åç§°
            
        Returns:
            APIå“åº”æ•°æ®æˆ–None
        """
        
        form_data = {
            'limit': str(limit),
            'current': str(current),
            'pubDateStartTime': pub_date_start_time,
            'pubDateEndTime': pub_date_end_time,
            'prodPcatid': prod_pcatid,
            'prodCatid': prod_catid,
            'prodName': prod_name
        }
        
        return self._make_request(form_data)
    
    def scrape_all_pages(self, 
                        limit: int = 20,
                        max_pages: Optional[int] = None,
                        save_callback: Optional[callable] = None,
                        **kwargs) -> List[Dict[str, Any]]:
        """æŠ“å–æ‰€æœ‰é¡µé¢æ•°æ®
        
        Args:
            limit: æ¯é¡µè®°å½•æ•°
            max_pages: æœ€å¤§é¡µæ•°é™åˆ¶
            save_callback: ä¿å­˜å›è°ƒå‡½æ•°ï¼Œå¦‚æœæä¾›åˆ™ç«‹å³ä¿å­˜æ¯é¡µæ•°æ®
            **kwargs: å…¶ä»–æŸ¥è¯¢å‚æ•°
            
        Returns:
            æ‰€æœ‰æ•°æ®è®°å½•åˆ—è¡¨ï¼ˆå¦‚æœä½¿ç”¨save_callbackï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼‰
        """
        
        all_records = []
        current_page = 1
        total_count = None
        total_pages = 0
        total_saved = 0
        
        # Debug: æ˜¾ç¤ºå½“å‰æŠ“å–è®¡åˆ’
        plan_info = f"ğŸ“‹ Current scraping plan: limit={limit}, max_pages={max_pages or 'unlimited'}"
        if kwargs:
            plan_info += f", filters={kwargs}"
        if save_callback:
            plan_info += ", immediate_save=True"
        logger.info(plan_info)
        logger.info("ğŸš€ Starting to scrape all pages data")
        
        while True:
            # Debug: æ˜¾ç¤ºå½“å‰é¡µé¢å’Œå‰©ä½™é¡µé¢ä¿¡æ¯
            if total_pages > 0:
                pages_left = total_pages - current_page
                logger.info(f"ğŸ“„ Current page: {current_page}/{total_pages} | Pages left: {pages_left}")
            else:
                logger.info(f"ğŸ“„ Current page: {current_page} (total pages unknown)")
            
            # æŠ“å–å½“å‰é¡µ
            result = self.scrape_page(limit=limit, current=current_page, **kwargs)
            
            if not result:
                logger.error(f"âŒ Page {current_page} scraping failed")
                break
            
            # è·å–æ€»è®°å½•æ•°ï¼ˆç¬¬ä¸€æ¬¡è¯·æ±‚æ—¶ï¼‰
            if total_count is None:
                total_count = result.get('count', 0)
                total_pages = (total_count + limit - 1) // limit
                logger.info(f"ğŸ“Š Total records: {total_count}, Total pages: {total_pages}")
                
                # å¦‚æœè®¾ç½®äº†æœ€å¤§é¡µæ•°é™åˆ¶
                if max_pages and total_pages > max_pages:
                    total_pages = max_pages
                    logger.info(f"âš ï¸  Limited max pages to: {max_pages}")
            
            # è·å–å½“å‰é¡µæ•°æ®
            page_records = result.get('list', [])
            
            # å¦‚æœæä¾›äº†ä¿å­˜å›è°ƒå‡½æ•°ï¼Œç«‹å³ä¿å­˜å½“å‰é¡µæ•°æ®
            if save_callback and page_records:
                try:
                    saved_count = save_callback(page_records)
                    total_saved += saved_count
                    logger.info(f"ğŸ’¾ Page {current_page} saved: {saved_count} records (total saved: {total_saved})")
                except Exception as e:
                    logger.error(f"âŒ Failed to save page {current_page} data: {e}")
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€é¡µï¼Œä¸ä¸­æ–­æ•´ä¸ªæŠ“å–è¿‡ç¨‹
            else:
                # å¦‚æœæ²¡æœ‰ä¿å­˜å›è°ƒï¼Œåˆ™ç¼“å­˜åˆ°å†…å­˜ä¸­
                all_records.extend(page_records)
            
            logger.info(f"âœ… Page {current_page} completed: {len(page_records)} records retrieved")
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µé¢
            if len(page_records) < limit:
                logger.info("ğŸ Reached the last page")
                break
            
            # æ£€æŸ¥é¡µæ•°é™åˆ¶
            if max_pages and current_page >= max_pages:
                logger.info(f"ğŸ›‘ Reached max pages limit: {max_pages}")
                break
            
            current_page += 1
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            logger.info("â³ Waiting 1 second before next request...")
            time.sleep(1)
        
        if save_callback:
            logger.info(f"ğŸ‰ Scraping completed! Total records saved: {total_saved}")
            return []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œå› ä¸ºæ•°æ®å·²ç»ä¿å­˜
        else:
            logger.info(f"ğŸ‰ Scraping completed! Total records retrieved: {len(all_records)}")
            return all_records
    
    def scrape_by_date_range(self, 
                           start_date: str, 
                           end_date: str,
                           limit: int = 20,
                           max_pages: Optional[int] = None,
                           save_callback: Optional[callable] = None) -> List[Dict[str, Any]]:
        """æŒ‰æ—¥æœŸèŒƒå›´æŠ“å–æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            limit: æ¯é¡µè®°å½•æ•°
            max_pages: æœ€å¤§é¡µæ•°é™åˆ¶
            save_callback: ä¿å­˜å›è°ƒå‡½æ•°ï¼Œå¦‚æœæä¾›åˆ™ç«‹å³ä¿å­˜æ¯é¡µæ•°æ®
            
        Returns:
            æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰æ•°æ®è®°å½•ï¼ˆå¦‚æœä½¿ç”¨save_callbackï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼‰
        """
        
        logger.info(f"ğŸ“… Scraping by date range: {start_date} to {end_date}")
        logger.info(f"ğŸ¯ Plan: Date range scraping with limit={limit}, max_pages={max_pages or 'unlimited'}")
        
        return self.scrape_all_pages(
            limit=limit,
            max_pages=max_pages,
            save_callback=save_callback,
            pub_date_start_time=start_date,
            pub_date_end_time=end_date
        )
    
    def scrape_by_product(self, 
                         prod_name: str = "",
                         prod_catid: str = "",
                         limit: int = 20,
                         max_pages: Optional[int] = None,
                         save_callback: Optional[callable] = None) -> List[Dict[str, Any]]:
        """æŒ‰äº§å“æŠ“å–æ•°æ®
        
        Args:
            prod_name: äº§å“åç§°
            prod_catid: äº§å“åˆ†ç±»ID
            limit: æ¯é¡µè®°å½•æ•°
            max_pages: æœ€å¤§é¡µæ•°é™åˆ¶
            save_callback: ä¿å­˜å›è°ƒå‡½æ•°ï¼Œå¦‚æœæä¾›åˆ™ç«‹å³ä¿å­˜æ¯é¡µæ•°æ®
            
        Returns:
            æŒ‡å®šäº§å“çš„æ‰€æœ‰æ•°æ®è®°å½•ï¼ˆå¦‚æœä½¿ç”¨save_callbackï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼‰
        """
        
        logger.info(f"ğŸ›’ Scraping by product: name='{prod_name}', category_id='{prod_catid}'")
        logger.info(f"ğŸ¯ Plan: Product scraping with limit={limit}, max_pages={max_pages or 'unlimited'}")
        
        return self.scrape_all_pages(
            limit=limit,
            max_pages=max_pages,
            save_callback=save_callback,
            prod_name=prod_name,
            prod_catid=prod_catid
        )
    
    def scrape_latest(self, days: int = 7, limit: int = 20, save_callback: Optional[callable] = None) -> List[Dict[str, Any]]:
        """æŠ“å–æœ€è¿‘å‡ å¤©çš„æ•°æ®
        
        Args:
            days: æœ€è¿‘å¤©æ•°
            limit: æ¯é¡µè®°å½•æ•°
            save_callback: ä¿å­˜å›è°ƒå‡½æ•°ï¼Œå¦‚æœæä¾›åˆ™ç«‹å³ä¿å­˜æ¯é¡µæ•°æ®
            
        Returns:
            æœ€è¿‘å‡ å¤©çš„æ‰€æœ‰æ•°æ®è®°å½•ï¼ˆå¦‚æœä½¿ç”¨save_callbackï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼‰
        """
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        start_date_str = start_date.strftime('%Y-%m-%d')
        end_date_str = end_date.strftime('%Y-%m-%d')
        
        logger.info(f"â° Scraping latest {days} days data: {start_date_str} to {end_date_str}")
        logger.info(f"ğŸ¯ Plan: Latest data scraping with limit={limit}")
        
        return self.scrape_by_date_range(start_date_str, end_date_str, limit, save_callback=save_callback)
    
    def close(self):
        """å…³é—­ä¼šè¯"""
        self.session.close()
        logger.info("æŠ“å–å™¨ä¼šè¯å·²å…³é—­")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
